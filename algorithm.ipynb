{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7961d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[0mSetting top to                           :\u001b[0m \u001b[0m\u001b[32m\u001b[32m/Users/sed_zeppelin/Master Project\u001b[0m \u001b[0m\n",
      "\u001b[32m\u001b[0mSetting out to                           :\u001b[0m \u001b[0m\u001b[32m\u001b[32m/Users/sed_zeppelin/Master Project/bin\u001b[0m \u001b[0m\n",
      "\u001b[32m\u001b[0mChecking for 'clang++' (C++ compiler)    :\u001b[0m \u001b[0m\u001b[32m\u001b[32m/usr/bin/clang++\u001b[0m \u001b[0m\n",
      "\u001b[32m\u001b[0mUnpacking gtest                          :\u001b[0m \u001b[0m\u001b[32m\u001b[32myes\u001b[0m \u001b[0m\n",
      "\u001b[32m\u001b[0mChecking for library pthread             :\u001b[0m \u001b[0m\u001b[32m\u001b[32myes\u001b[0m \u001b[0m\n",
      "\u001b[32m'configure' finished successfully (0.545s)\u001b[0m\n",
      "\u001b[32mWaf: Entering directory `/Users/sed_zeppelin/Master Project/bin'\u001b[0m\n",
      "\u001b[32mWaf: Leaving directory `/Users/sed_zeppelin/Master Project/bin'\u001b[0m\n",
      "\u001b[32m'build' finished successfully (0.110s)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', './waf-2.0.24'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the source code\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "subprocess.run(shlex.split('python ./waf-2.0.24 configure'))\n",
    "subprocess.run(shlex.split('python ./waf-2.0.24'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d1364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the shell command to a python method -runAlgorithm\n",
    "\n",
    "\n",
    "# import subprocess\n",
    "# import shlex\n",
    "#  'random_seed': '114514'\n",
    "\n",
    "def runAlgorithm(**kwargs):\n",
    "    \n",
    "    # Default arguments of the method:\n",
    "    default_args = {'type': 'gen', 'graph': \"flower 1000 1 2\", 'method': 'sketch', 'alpha': '1', 'least_coverage': '1',\n",
    "           'sketch_k': '128', 'multipass': '10000', 'rad_min': '1', 'rad_max': '30' , 'random_seed': '42'}\n",
    "    \n",
    "    options = \"\"\n",
    "    \n",
    "    for key in kwargs:\n",
    "        default_args[key] = kwargs[key]\n",
    "        \n",
    "    if default_args['type'] == 'gen':\n",
    "        a = default_args['graph']\n",
    "        default_args['graph'] = f'\"{a}\"'\n",
    "            \n",
    "    for key in default_args:\n",
    "        options += f\" -{key}={default_args[key]}\"\n",
    "        \n",
    "    subprocess.run(shlex.split(f\"./bin/box_cover {options}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151fc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./coaut.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf0bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing APS dataset for the experiments:\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import csv\n",
    "from graph_tool.all import *\n",
    "\n",
    "\n",
    "def get_aps (data, begin_time, end_time, journal):\n",
    "    \n",
    "    \n",
    "    # Shrinking the data according to the journal:\n",
    "    \n",
    "    df = data.loc[data['domain'] == journal]\n",
    "\n",
    "              \n",
    "    # Removing self-loops and duplicate edges:\n",
    "    \n",
    "    df = df[df['fact_u'] > df['fact_v']]\n",
    "    \n",
    "    \n",
    "    # Removing parallel edges:\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['fact_u', 'fact_v'])\n",
    "    \n",
    "    # Shrinking the data according to the time period:\n",
    "    \n",
    "    timelist = df['time'].tolist()\n",
    "    for i in range(len(timelist)):\n",
    "        timelist[i] = timelist[i][:7]\n",
    "    index1 = timelist.index(begin_time)\n",
    "    timelist.reverse()\n",
    "    index2 = timelist.index(end_time)\n",
    "    index2 = len(timelist) - index2\n",
    "    \n",
    "    df = df.iloc[index1: index2]\n",
    "       \n",
    "    \n",
    "    # Removing outliers:\n",
    "\n",
    "    def get_no_of_authors(cell): # For getting the number of authors for each row\n",
    "        return round(1/cell)\n",
    "    \n",
    "    df['no_authors'] = df['weight'].apply(get_no_of_authors) # Making the number-of-authors column\n",
    "    \n",
    "    std = df['no_authors'].std()\n",
    "    mean = df['no_authors'].mean()\n",
    "    \n",
    "    x_std = 2 * std\n",
    "    df = df[df['no_authors'] < mean + x_std]\n",
    "    del df['no_authors']\n",
    "    \n",
    "    \n",
    "    # Extracting the edges:\n",
    "    \n",
    "    df2 = df[['fact_u', 'fact_v']]\n",
    "    edgelist = df2.values.tolist()\n",
    "    \n",
    "    \n",
    "    # Re-labelling edgelist:\n",
    "    \n",
    "    temp_set = set()\n",
    "    for edge in edgelist:\n",
    "        for vertex in edge:\n",
    "            temp_set.add(vertex)\n",
    "    \n",
    "    temp_list = list(temp_set)\n",
    "    vertex_mapping = {} # old_label -> new_label\n",
    "    for i in range(len(temp_list)):\n",
    "        vertex_mapping[temp_list[i]] = i\n",
    "\n",
    "    for i in range(len(edgelist)):\n",
    "        for j in range(2):\n",
    "            edgelist[i][j] = vertex_mapping[edgelist[i][j]]\n",
    "      \n",
    "    \n",
    "    # Getting the largest connected component and writing it to file:\n",
    "    \n",
    "    G = Graph(directed=False)\n",
    "    G.add_edge_list(edgelist)\n",
    "    \n",
    "    largest_comp = GraphView(G, vfilt = label_largest_component(G))\n",
    "    \n",
    "    edges = np.array(largest_comp.get_edges())\n",
    "    np.savetxt('./apstest', edges, fmt='%d', delimiter = '\\t')\n",
    "    \n",
    "    \n",
    "    return edgelist, largest_comp, G\n",
    "\n",
    "# aps = get_aps(df, '1993-01', '1993-12', 'PRE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed79db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# The function for reading size and radius from the output file:\n",
    "\n",
    "def get_size_radius():\n",
    "    \n",
    "    list_of_files = glob.glob('./jlog/*')\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "    f = open(latest_file, \"r\").read()\n",
    "    json_file = json.loads(f)\n",
    "\n",
    "    radius = json_file['radius']\n",
    "    size = json_file['size']\n",
    "\n",
    "    print (f\"radius: \\n{radius}\\n\")\n",
    "    print (f\"size: \\n{size}\")\n",
    "\n",
    "    return size, radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6726ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "# The function for fitting the lines to the data:\n",
    "\n",
    "def fit(get_size_radius):\n",
    "    \n",
    "    lbs = get_size_radius[1]\n",
    "    nbs = get_size_radius[0]\n",
    "    \n",
    "    x = np.array(lbs)\n",
    "    y = np.array(nbs)\n",
    "    x_log = np.log10(lbs)\n",
    "    y_log = np.log10(nbs)\n",
    "    \n",
    "    fit_pl = linregress(x_log, y_log)\n",
    "    fit_exp = linregress(x, y_log)\n",
    "    \n",
    "    y_log_pl = fit_pl.slope * x_log + fit_pl.intercept\n",
    "    y_log_exp = fit_exp.slope * x + fit_exp.intercept\n",
    "    \n",
    "    return fit_pl.slope, fit_exp.slope, fit_pl, fit_exp, x_log, y_log, y_log_pl, y_log_exp, nbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1a97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import graph_tool.all as gt\n",
    "\n",
    "\n",
    "# The function for getting the average shortest path length, average clustering coefficient, and network diameter:\n",
    "\n",
    "def statistics1(G):\n",
    "        \n",
    "    network_diameter = int(distance_histogram(G)[1][-1]-1)\n",
    "    \n",
    "    dist = gt.shortest_distance(G)\n",
    "    average_shortest_path_length = sum([sum(i) for i in dist])/(G.num_vertices()**2-G.num_vertices())\n",
    "    average_culstering_coefficient = sum(local_clustering(G))/G.num_vertices()\n",
    "    \n",
    "    return average_shortest_path_length, average_culstering_coefficient, network_diameter\n",
    "\n",
    "\n",
    "\n",
    "# The function for getting susceptibility, correlation length, percolation probability, n_nodes, n_edges and avg_degree:\n",
    "\n",
    "def statistics2(G):\n",
    "    \n",
    "    n_nodes = G.num_vertices()\n",
    "    n_edges = G.num_edges()\n",
    "    \n",
    "    comp, hist = label_components(G)\n",
    "    comp_size = list(hist)\n",
    "    comp_size.sort(reverse=True)\n",
    "    comp_size_freq = np.array(list(dict(Counter(comp_size)).items()))\n",
    "    comp_size_freq = comp_size_freq[np.argsort(comp_size_freq[:, 0])]\n",
    "    # percolation probability\n",
    "    p = comp_size[0]/n_nodes\n",
    "    # susceptibility\n",
    "    s = 0\n",
    "    for i in range(len(comp_size_freq)-1):\n",
    "        s += comp_size_freq[i, 0]**2*comp_size_freq[i, 1]/n_nodes\n",
    "    if s == 0:\n",
    "        s = np.nan\n",
    "    # correlation length\n",
    "    xi = comp_size[1]\n",
    "         \n",
    "    degrees = G.degree_property_map(\"total\")\n",
    "    avg_degree = sum(degrees) / n_nodes\n",
    "    \n",
    "    return s, xi, p, avg_degree, n_nodes, n_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "385662f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final function that gets the time period and journal, and returns fractality and biggest component:\n",
    "\n",
    "def f(begin_time, end_time, journal):\n",
    "    \n",
    "    largets_connected_component = get_aps(df, begin_time, end_time, journal)[1]\n",
    "    graph = get_aps(df, begin_time, end_time, journal)[2]\n",
    "    runAlgorithm (type = 'tsv', graph = './apstest')\n",
    "    size, radius = get_size_radius()\n",
    "    \n",
    "    return size, radius, largets_connected_component, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7613658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_labels(starting_from: str, n: int): # starting_from should be in the form of \"year-month\"\n",
    "    \n",
    "    year = int(starting_from.split('-')[0])\n",
    "    month = starting_from.split('-')[1]\n",
    "    \n",
    "    months_list = ['03', '06', '09', '12']\n",
    "    \n",
    "    position_in_months_list = months_list.index(month)\n",
    "    \n",
    "    time_windows = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        if i > 0 and (i + position_in_months_list)%4 == 0:\n",
    "            year = year + 1\n",
    "        time_windows.append(str(year) + '-' + str(months_list[(i + position_in_months_list)%4]))\n",
    "    \n",
    "    return (time_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4fc394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_from(date_end):\n",
    "    \n",
    "    months_list1 = ['03', '06', '09', '12']\n",
    "    months_list2 = ['04', '07', '10', '01']\n",
    "    \n",
    "    year = date_end.split('-')[0]\n",
    "    month1 = date_end.split('-')[1]\n",
    "    month2 = months_list2[months_list1.index(month1)]\n",
    "    \n",
    "    if months_list1.index(month1) == 3:\n",
    "        year = str(int(year)//10)+str(int(year)%10+1)\n",
    "    \n",
    "    return year + '-' + month2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "000b8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_window(date_end):\n",
    "    \n",
    "    months_list = ['03', '06', '09', '12', '03']\n",
    "    \n",
    "    year = date_end.split('-')[0]\n",
    "    month1 = date_end.split('-')[1]\n",
    "    month2 = months_list[months_list.index(month1)+1]\n",
    "    \n",
    "    if months_list.index(month1) == 3:\n",
    "        year = str(int(year)//10)+str(int(year)%10+1)\n",
    "    \n",
    "    return year + '-' + month2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e429db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "\n",
    "def run_snapshot(first_window, aggregate_from, number_of_windows, journal, snapshot_number):\n",
    "\n",
    "    begin_time1 = datetime.datetime.now()\n",
    "\n",
    "    strs = get_windows_labels(first_window, number_of_windows)\n",
    "\n",
    "    avg_shortest_path_len_m3 = []\n",
    "    avg_clus_coef_m3 = []\n",
    "    net_diameter_m3 = []\n",
    "\n",
    "    susceptibility_3m = []\n",
    "    corrlength_3m = []\n",
    "    percolation_3m = []\n",
    "\n",
    "    avg_degree_3m = []\n",
    "    n_nodes_3m = []\n",
    "    n_edges_3m = []\n",
    "    \n",
    "    fractal_dimension_3m = []\n",
    "    error_ratio_3m = []\n",
    "\n",
    "    for i in range(len(strs)):\n",
    "\n",
    "        print (f'iteration {i}')\n",
    "\n",
    "        begin_time2 = datetime.datetime.now()\n",
    "\n",
    "        aps = get_aps(df, aggregate_from, strs[i], journal)\n",
    "        stats1 = statistics1(aps[1])\n",
    "        avg_shortest_path_len_m3.append(stats1[0])\n",
    "        avg_clus_coef_m3.append(stats1[1])\n",
    "        net_diameter_m3.append(stats1[2])\n",
    "        \n",
    "        stats2 = statistics2(aps[2])\n",
    "        susceptibility_3m.append(stats2[0])\n",
    "        corrlength_3m.append(stats2[1])\n",
    "        percolation_3m.append(stats2[2])\n",
    "        avg_degree_3m.append(stats2[3])\n",
    "        n_nodes_3m.append(stats2[4])\n",
    "        n_edges_3m.append(stats2[5])\n",
    "        \n",
    "        F = f(aggregate_from, strs[i], journal)\n",
    "        \n",
    "        if len(F[0]) < 4:\n",
    "            fractal_dimension_3m.append(math.nan)\n",
    "            error_ratio_3m.append(math.nan)\n",
    "        else:\n",
    "            fitted = fit(F)\n",
    "            fractal_dimension_3m.append(-1 * fitted[0])\n",
    "            wmse_exp_3m = sum(fitted[8] * (fitted[7] - fitted[5]) ** 2) / sum(fitted[8]) / len(fitted[7])\n",
    "            wmse_pl_3m = sum(fitted[8] * (fitted[6] - fitted[5]) ** 2) / sum(fitted[8]) / len(fitted[6])\n",
    "            error_ratio_3m.append(wmse_exp_3m/wmse_pl_3m)\n",
    "            \n",
    "        end_time2 = datetime.datetime.now()\n",
    "        runtime2 = end_time2 - begin_time2\n",
    "        print (f'runtime: {runtime2}')\n",
    "        \n",
    "    dataframe = pd.DataFrame(data=[avg_shortest_path_len_m3, avg_clus_coef_m3, net_diameter_m3, susceptibility_3m, corrlength_3m, percolation_3m, avg_degree_3m, n_nodes_3m, n_edges_3m, fractal_dimension_3m, error_ratio_3m, strs]).transpose()\n",
    "    dataframe.columns = ['avg_shortest_path_len_m3', 'avg_clus_coef_m3', 'net_diameter_m3', 'susceptibility_3m', 'corrlength_3m', 'percolation_3m', 'avg_degree_3m', 'n_nodes_3m', 'n_edges_3m', 'fractal_dimension_3m', 'error_ratio_3m', 'time_window']\n",
    "    dataframe.to_csv('./PRE/snapshot'+str(snapshot_number)+'.csv')\n",
    "    \n",
    "    t_end = susceptibility_3m.index(max(susceptibility_3m))\n",
    "    date_end = strs[t_end]\n",
    "    \n",
    "    end_time1 = datetime.datetime.now()\n",
    "    runtime1 = end_time1 - begin_time1\n",
    "    print (f'runtime: {runtime1}')\n",
    "    \n",
    "    return t_end, date_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86bc483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aggregate_from = '1993-01'\n",
    "first_window = '1993-03'\n",
    "number_of_windows = 30\n",
    "snapshot_number = 1\n",
    "journal = 'PRE'\n",
    "\n",
    "t_ends = []\n",
    "end_dates = []\n",
    "\n",
    "all_window_labels = get_windows_labels('1993-03', 68)\n",
    "\n",
    "while_loop_begin_time = datetime.datetime.now()\n",
    "\n",
    "while int(first_window.split('-')[0]) < 2007:\n",
    "    \n",
    "    peak = run_snapshot(first_window, aggregate_from, number_of_windows, journal, snapshot_number)\n",
    "    t_ends.append(peak[0])\n",
    "    end_dates.append(peak[1])\n",
    "    \n",
    "    print (f'snapshot {snapshot_number} done!')\n",
    "    print (f't_end: {peak[0]}')\n",
    "    print (f'date_end: {peak[1]}\\n')\n",
    "    \n",
    "    aggregate_from = get_aggregate_from(peak[1])\n",
    "    first_window = get_first_window(peak[1])\n",
    "    snapshot_number = snapshot_number + 1\n",
    "    \n",
    "    if len(all_window_labels) - all_window_labels.index(first_window) < number_of_windows:\n",
    "        number_of_windows = len(all_window_labels) - all_window_labels.index(first_window)\n",
    "\n",
    "while_loop_end_time = datetime.datetime.now()\n",
    "while_loop_runtime = while_loop_end_time - while_loop_begin_time\n",
    "print (f'runtime: {while_loop_runtime}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94feb879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(filename, data_list):\n",
    "    with open(\"./\"+journal+\"/\" + filename, 'w') as file:\n",
    "        for item in data_list:\n",
    "            file.write(str(item) + '\\n')\n",
    "\n",
    "# def read_list_from_file(filename):\n",
    "#     data_list = []\n",
    "#     with open(\"./PRE/\" + filename, 'r') as file:\n",
    "#         for line in file:\n",
    "#             data_list.append(int(line.strip()))\n",
    "#     return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_list_to_file('t_ends', t_ends)\n",
    "write_list_to_file('end_dates', end_dates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
